{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "imdb, info= tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#25000 for training and 25000 for testing\n",
    "train_data, test_data= imdb['train'], imdb['test']\n",
    "\n",
    "training_sentences= []\n",
    "training_labels=[]\n",
    "testing_sentences= []\n",
    "testing_labels= []\n",
    "\n",
    "#label=1 represents positive review and label=1 ==> -ve review\n",
    "#each loops over 25000 iterables that contain sentences and labels as TENSORS\n",
    "for s,l in train_data:\n",
    "    #here s and l are tensors so before appending into lists we convert them into \n",
    "    #numpyBYTES and numpyINT respectively\n",
    "    training_sentences.append(str(s.numpy()))\n",
    "    training_labels.append(l.numpy())\n",
    "for s,l in test_data:\n",
    "    testing_sentences.append(str(s.numpy()))\n",
    "    testing_labels.append(l.numpy())\n",
    "\n",
    "training_labels_final= np.array(training_labels)\n",
    "testing_labels_final= np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 120)\n"
     ]
    }
   ],
   "source": [
    "#PREPROCESSING DATA I.E. converting text into sequence of numbers using Tokenizer\n",
    "vocab_size=10000    #assuming max unique words 10000. if there are more than 10000 words they will be ignored\n",
    "embedding_dim=16    #basically vector (word representation) will be in 16 dimensions\n",
    "max_length=120      #max width of padded matrix (i.e. truncated width of each sentence)\n",
    "trunc_type='post'\n",
    "oov_tok= \"<OOV>\"   #token given to words in sequence which are not encountered before while fitting\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#TOKENIZER FITTING ON TRAINING TEXTS ONLY ###################\n",
    "tokenizer= Tokenizer(num_words= vocab_size, oov_token= oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "#word index returns dictionary with key->word, value->token given to that word\n",
    "word_index= tokenizer.word_index\n",
    "#getting encoded code for sentences\n",
    "sequences= tokenizer.texts_to_sequences(training_sentences)\n",
    "\n",
    "\n",
    "#input size feeded to NN should be uniform. For that PADDING is done!\n",
    "#list of sentences have been padded out into sentences\n",
    "\n",
    "#matrix width = length of longest sentence. you can overwrite that with 'maxlen' parameter\n",
    "#eg:- if you want your sentences to have only 5 words then maxlen=5. it truncates from beginning (pre)\n",
    "#you can change that to 'post' to truncate the words from last\n",
    "padded= pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "print(padded.shape)\n",
    "\n",
    "testing_sequences= tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded= pad_sequences(testing_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? ? ? ? ? ? ? b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the <OOV> and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own <OOV> without any real concern for anything else i cant recommend this film at all '\n",
      "b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index= dict([(value,key) for (key,value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "#decode review this is what is fed in NN\n",
    "print(decode_review(padded[1]))\n",
    "print(training_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#DEFINING THE MODEL\n",
    "#here LSTM and GRU are type RNN. Try using them one by one in place of Flatten layer\n",
    "import tensorflow as tf\n",
    "model= tf.keras.Sequential([ \n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    #tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
    "    tf.keras.layers.Dense(6, activation= 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation= 'sigmoid')])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 4s 177us/sample - loss: 0.4852 - accuracy: 0.7551 - val_loss: 0.3458 - val_accuracy: 0.8512\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 4s 158us/sample - loss: 0.2490 - accuracy: 0.9011 - val_loss: 0.3607 - val_accuracy: 0.8414\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 4s 159us/sample - loss: 0.1133 - accuracy: 0.9669 - val_loss: 0.4361 - val_accuracy: 0.8292\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 4s 160us/sample - loss: 0.0314 - accuracy: 0.9956 - val_loss: 0.5186 - val_accuracy: 0.8259\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 4s 165us/sample - loss: 0.0089 - accuracy: 0.9990 - val_loss: 0.6005 - val_accuracy: 0.8240\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 4s 178us/sample - loss: 0.0029 - accuracy: 0.9998 - val_loss: 0.6501 - val_accuracy: 0.8244\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 4s 170us/sample - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.6939 - val_accuracy: 0.8273\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 4s 174us/sample - loss: 6.1252e-04 - accuracy: 1.0000 - val_loss: 0.7364 - val_accuracy: 0.8275\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 4s 162us/sample - loss: 3.2735e-04 - accuracy: 1.0000 - val_loss: 0.7775 - val_accuracy: 0.8271\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 4s 164us/sample - loss: 1.9226e-04 - accuracy: 1.0000 - val_loss: 0.8158 - val_accuracy: 0.8272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2aa8b76b38>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs=10\n",
    "model.fit(padded, training_labels_final, epochs= num_epochs, validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01769426 -0.01639816 -0.01922341 ... -0.02487399 -0.02925328\n",
      "   0.01707141]\n",
      " [ 0.06581753 -0.05216534 -0.02537935 ... -0.05186357  0.00225915\n",
      "  -0.02025491]\n",
      " [ 0.01910185 -0.12874885 -0.03501788 ... -0.06317361  0.03781614\n",
      "  -0.07741185]\n",
      " ...\n",
      " [ 0.01552939 -0.18754904  0.0100338  ... -0.0508533  -0.0367876\n",
      "   0.09208517]\n",
      " [ 0.00716479 -0.07919207  0.04087885 ...  0.02757098  0.05401395\n",
      "   0.05520029]\n",
      " [ 0.06409617 -0.06542336  0.07535129 ...  0.13620397 -0.0413507\n",
      "   0.02367678]]\n",
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "#what we'll do to feel this in embeding projector\n",
    "e=model.layers[0]   #taking output of my Embedding layer\n",
    "weights= e.get_weights()[0]\n",
    "print(weights.shape)  #shape= (vocab_size, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06581753 -0.05216534 -0.02537935 -0.12395547 -0.05069927  0.0150993\n",
      "  0.08432467 -0.04918864 -0.02570084 -0.05915948  0.03602956 -0.02155136\n",
      " -0.01719932 -0.05186357  0.00225915 -0.02025491]\n"
     ]
    }
   ],
   "source": [
    "#to get value of 16 dimensions for each word and write that in \"out_v\"\n",
    "#\"out_m\" contains actual word associated with that 16D vector\n",
    "import io\n",
    "\n",
    "out_v= io.open('vecs.tsv', 'w', encoding= 'utf-8')\n",
    "out_m= io.open('meta.tsv', 'w', encoding= 'utf-8')\n",
    "for word_num in range(1, vocab_size):\n",
    "    word= reverse_word_index[word_num]\n",
    "    embeddings= weights[word_num]\n",
    "    out_m.write(word + \"\\a\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"now vecs.tsv and meta.tsv have been made into your directory on PC\n",
    "now go to link- 'projector.tensorflow.org'\n",
    "then from left panel load vector file (vector for each word) and \n",
    "meta file (actual word). then see clusters have been formed for +ve\n",
    "and -ve reveiw words\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
